{
  "id": 42,
  "topic": "Python",
  "level": "Senior Level",
  "question": "Design a time-series database for storing metrics efficiently",
  "options": [
    "Store each point individually: high storage cost",
    "Use downsampling with aggregation: reduces accuracy",
    "Use compression (delta encoding): O(1) space per point",
    "Hybrid: recent detailed, old aggregated"
  ],
  "correct": 3,
  "explanation": "**Optimal Solution: Hybrid with Compression - Balanced approach**\n\n**Problem:** Store millions of time-series data points efficiently\n\n**Time-Series Characteristics:**\n```\n1. Sequential timestamps\n2. Often similar consecutive values\n3. Recent data queried more\n4. Historical data can be aggregated\n5. Write-heavy, read-optimized\n```\n\n**Storage Strategies:**\n```\nRaw storage:\nTimestamp: 8 bytes (int64)\nValue: 8 bytes (float64)\nTotal: 16 bytes per point\n\n1M points = 16 MB\n1B points = 16 GB\n\nWith compression:\n~2-4 bytes per point\n1B points = 2-4 GB (75% savings!)\n```\n\n**Visual Data Layout:**\n```\nRecent (detailed):\n10:00:00 → 100.5\n10:00:01 → 100.6\n10:00:02 → 100.7\n...\n\nOld (aggregated):\n09:00-10:00 → min=98.2, max=105.3, avg=101.2\n08:00-09:00 → min=95.1, max=103.8, avg=99.5\n```\n\n**Python Implementation:**\n```python\nimport time\nfrom collections import defaultdict, deque\nimport struct\n\nclass TimeSeriesDB:\n    def __init__(self, retention_seconds=3600):\n        self.retention = retention_seconds\n        # metric_name -> list of (timestamp, value)\n        self.raw_data = defaultdict(deque)\n        # metric_name -> bucket_time -> (min, max, sum, count)\n        self.aggregated = defaultdict(dict)\n        self.bucket_size = 60  # 1 minute buckets\n    \n    def write(self, metric_name, timestamp, value):\n        \"\"\"Write a data point\"\"\"\n        # Add to raw data\n        self.raw_data[metric_name].append((timestamp, value))\n        \n        # Update aggregated bucket\n        bucket_time = (timestamp // self.bucket_size) * self.bucket_size\n        \n        if bucket_time not in self.aggregated[metric_name]:\n            self.aggregated[metric_name][bucket_time] = {\n                'min': value,\n                'max': value,\n                'sum': value,\n                'count': 1\n            }\n        else:\n            agg = self.aggregated[metric_name][bucket_time]\n            agg['min'] = min(agg['min'], value)\n            agg['max'] = max(agg['max'], value)\n            agg['sum'] += value\n            agg['count'] += 1\n        \n        # Cleanup old data\n        self._cleanup(metric_name)\n    \n    def _cleanup(self, metric_name):\n        \"\"\"Remove data outside retention window\"\"\"\n        current_time = time.time()\n        cutoff = current_time - self.retention\n        \n        # Clean raw data\n        while (self.raw_data[metric_name] and \n               self.raw_data[metric_name][0][0] < cutoff):\n            self.raw_data[metric_name].popleft()\n        \n        # Clean aggregated data\n        old_buckets = [\n            t for t in self.aggregated[metric_name]\n            if t < cutoff\n        ]\n        for bucket in old_buckets:\n            del self.aggregated[metric_name][bucket]\n    \n    def query_range(self, metric_name, start_time, end_time):\n        \"\"\"Query data points in time range\"\"\"\n        result = []\n        \n        for timestamp, value in self.raw_data[metric_name]:\n            if start_time <= timestamp <= end_time:\n                result.append((timestamp, value))\n        \n        return result\n    \n    def query_aggregated(self, metric_name, start_time, end_time):\n        \"\"\"Query aggregated data\"\"\"\n        result = []\n        \n        for bucket_time, agg in self.aggregated[metric_name].items():\n            if start_time <= bucket_time <= end_time:\n                result.append({\n                    'timestamp': bucket_time,\n                    'min': agg['min'],\n                    'max': agg['max'],\n                    'avg': agg['sum'] / agg['count'],\n                    'count': agg['count']\n                })\n        \n        return sorted(result, key=lambda x: x['timestamp'])\n\n# Advanced: Delta Encoding Compression\nclass CompressedTimeSeries:\n    def __init__(self):\n        self.base_timestamp = None\n        self.base_value = None\n        self.deltas = []  # Store deltas instead of absolute values\n    \n    def append(self, timestamp, value):\n        \"\"\"Append with delta encoding\"\"\"\n        if self.base_timestamp is None:\n            self.base_timestamp = timestamp\n            self.base_value = value\n            return\n        \n        # Store delta from previous\n        time_delta = timestamp - self.base_timestamp\n        value_delta = value - self.base_value\n        \n        self.deltas.append((time_delta, value_delta))\n        \n        # Update base for next delta\n        self.base_timestamp = timestamp\n        self.base_value = value\n    \n    def get_all(self):\n        \"\"\"Decompress all points\"\"\"\n        if self.base_timestamp is None:\n            return []\n        \n        result = [(self.base_timestamp, self.base_value)]\n        current_time = self.base_timestamp\n        current_value = self.base_value\n        \n        for time_delta, value_delta in self.deltas:\n            current_time += time_delta\n            current_value += value_delta\n            result.append((current_time, current_value))\n        \n        return result\n\n# Advanced: Gorilla-style compression\nclass GorillaCompression:\n    \"\"\"Facebook's Gorilla TSDB compression\"\"\"\n    \n    @staticmethod\n    def compress_timestamp(timestamps):\n        \"\"\"Delta-of-delta encoding for timestamps\"\"\"\n        if len(timestamps) < 2:\n            return timestamps\n        \n        compressed = [timestamps[0]]\n        prev_delta = timestamps[1] - timestamps[0]\n        compressed.append(prev_delta)\n        \n        for i in range(2, len(timestamps)):\n            delta = timestamps[i] - timestamps[i-1]\n            delta_of_delta = delta - prev_delta\n            compressed.append(delta_of_delta)\n            prev_delta = delta\n        \n        return compressed\n    \n    @staticmethod\n    def compress_values(values):\n        \"\"\"XOR-based compression for floats\"\"\"\n        if len(values) < 2:\n            return values\n        \n        compressed = [values[0]]\n        prev = values[0]\n        \n        for val in values[1:]:\n            # XOR with previous value\n            xor_result = struct.unpack('Q', \n                struct.pack('d', val ^ prev))[0]\n            compressed.append(xor_result)\n            prev = val\n        \n        return compressed\n\n# Production-ready: Multi-level storage\nclass ProductionTimeSeriesDB:\n    def __init__(self):\n        # Level 1: Recent data (last hour) - full resolution\n        self.recent = defaultdict(deque)\n        self.recent_window = 3600\n        \n        # Level 2: Medium-term (last day) - 1min aggregates\n        self.hourly = defaultdict(dict)\n        \n        # Level 3: Long-term (30 days) - 1hour aggregates\n        self.daily = defaultdict(dict)\n    \n    def write(self, metric, timestamp, value):\n        \"\"\"Write with automatic tiering\"\"\"\n        # Add to recent\n        self.recent[metric].append((timestamp, value))\n        \n        # Aggregate for hourly\n        hour_bucket = (timestamp // 3600) * 3600\n        if hour_bucket not in self.hourly[metric]:\n            self.hourly[metric][hour_bucket] = []\n        self.hourly[metric][hour_bucket].append(value)\n        \n        # Cleanup\n        self._age_out_data(metric, timestamp)\n    \n    def _age_out_data(self, metric, current_time):\n        \"\"\"Move old data to lower resolution tiers\"\"\"\n        # Age out from recent to hourly\n        cutoff = current_time - self.recent_window\n        while (self.recent[metric] and \n               self.recent[metric][0][0] < cutoff):\n            self.recent[metric].popleft()\n    \n    def query(self, metric, start, end):\n        \"\"\"Smart query across tiers\"\"\"\n        current = time.time()\n        \n        # Recent data (detailed)\n        if end >= current - self.recent_window:\n            return self._query_recent(metric, start, end)\n        \n        # Historical data (aggregated)\n        return self._query_aggregated(metric, start, end)\n    \n    def _query_recent(self, metric, start, end):\n        result = []\n        for ts, val in self.recent[metric]:\n            if start <= ts <= end:\n                result.append((ts, val))\n        return result\n    \n    def _query_aggregated(self, metric, start, end):\n        result = []\n        for bucket, values in self.hourly[metric].items():\n            if start <= bucket <= end:\n                result.append({\n                    'timestamp': bucket,\n                    'avg': sum(values) / len(values),\n                    'min': min(values),\n                    'max': max(values)\n                })\n        return result\n```\n\n**Complexity:**\n- Write: O(1) amortized\n- Query: O(k) where k = points in range\n- Space: O(n) with compression reducing constant factor\n\n**Key Insight:** Hybrid approach balances query speed and storage; recent data detailed for precision, old data aggregated for efficiency; compression techniques like delta encoding save 50-75% space.",
  "timeComplexity": "O(1) write, O(k) query",
  "spaceComplexity": "O(n) with compression",
  "videoUrl": "https://www.youtube.com/watch?v=kpynZHRUZ3I",
  "videoTitle": "Gorilla: A Fast, Scalable, In-Memory Time Series Database"
}
