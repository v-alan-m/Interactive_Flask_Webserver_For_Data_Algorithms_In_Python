{
  "id": 32,
  "topic": "Python",
  "level": "Senior Level",
  "question": "Design a time-series database for storing metrics efficiently",
  "options": [
    "Store all points in list: O(n) queries",
    "Use dict by timestamp: O(1) lookup, O(n) range",
    "Use SortedDict: O(log n) insert, O(k) range queries",
    "Use pandas DataFrame: convenient but memory-heavy"
  ],
  "correct": 2,
  "explanation": "**Optimal Solution: SortedDict - O(log n) insert, O(k) range**\n\n**Problem:** Store time-series metrics with operations:\n```\n- write(timestamp, value): store data point\n- read(timestamp): get value at specific time\n- rangeQuery(start, end): get all values in range\n- aggregate(start, end): compute avg/sum/max in range\n```\n\n**Visual Example:**\n```\nTime Series Data:\nTimestamp  Value\n1000       10\n1010       15\n1020       12\n1030       18\n1040       20\n\nrangeQuery(1010, 1030):\nReturn: [(1010, 15), (1020, 12), (1030, 18)]\n```\n\n**Why SortedDict?**\n```\nSortedDict (from sortedcontainers):\n- Maintains sorted order by timestamp\n- Binary search: O(log n)\n- Range queries: O(log n + k) where k = results\n- Memory efficient\n- Compression friendly\n```\n\n**Python Implementation:**\n```python\nfrom sortedcontainers import SortedDict\nfrom collections import defaultdict\nimport bisect\n\nclass TimeSeriesDB:\n    def __init__(self):\n        # SortedDict: timestamp -> value\n        self.data = SortedDict()\n    \n    def write(self, timestamp, value):\n        \"\"\"Write a data point: O(log n)\"\"\"\n        self.data[timestamp] = value\n    \n    def read(self, timestamp):\n        \"\"\"Read exact timestamp: O(log n)\"\"\"\n        return self.data.get(timestamp)\n    \n    def rangeQuery(self, start_time, end_time):\n        \"\"\"Get all points in range: O(log n + k)\"\"\"\n        result = []\n        \n        # Binary search for start position\n        start_idx = self.data.bisect_left(start_time)\n        end_idx = self.data.bisect_right(end_time)\n        \n        # Get keys in range\n        keys = self.data.keys()[start_idx:end_idx]\n        \n        for key in keys:\n            result.append((key, self.data[key]))\n        \n        return result\n    \n    def aggregate(self, start_time, end_time, func='avg'):\n        \"\"\"Compute aggregate in range: O(log n + k)\"\"\"\n        points = self.rangeQuery(start_time, end_time)\n        \n        if not points:\n            return None\n        \n        values = [v for t, v in points]\n        \n        if func == 'avg':\n            return sum(values) / len(values)\n        elif func == 'sum':\n            return sum(values)\n        elif func == 'max':\n            return max(values)\n        elif func == 'min':\n            return min(values)\n        elif func == 'count':\n            return len(values)\n    \n    def latest(self, n=1):\n        \"\"\"Get n most recent points: O(n)\"\"\"\n        if n >= len(self.data):\n            return list(self.data.items())\n        return list(self.data.items())[-n:]\n    \n    def downsample(self, interval):\n        \"\"\"Downsample to specified interval: O(n)\"\"\"\n        if not self.data:\n            return []\n        \n        result = []\n        current_bucket = []\n        bucket_start = min(self.data.keys())\n        \n        for timestamp, value in self.data.items():\n            if timestamp < bucket_start + interval:\n                current_bucket.append(value)\n            else:\n                # Aggregate current bucket (avg)\n                if current_bucket:\n                    avg_val = sum(current_bucket) / len(current_bucket)\n                    result.append((bucket_start, avg_val))\n                \n                # Start new bucket\n                bucket_start = timestamp\n                current_bucket = [value]\n        \n        # Add last bucket\n        if current_bucket:\n            avg_val = sum(current_bucket) / len(current_bucket)\n            result.append((bucket_start, avg_val))\n        \n        return result\n\n# Multi-metric time series\nclass MultiMetricTSDB:\n    def __init__(self):\n        # metric_name -> SortedDict(timestamp -> value)\n        self.metrics = defaultdict(SortedDict)\n    \n    def write(self, metric, timestamp, value):\n        self.metrics[metric][timestamp] = value\n    \n    def rangeQuery(self, metric, start_time, end_time):\n        data = self.metrics[metric]\n        start_idx = data.bisect_left(start_time)\n        end_idx = data.bisect_right(end_time)\n        keys = data.keys()[start_idx:end_idx]\n        return [(key, data[key]) for key in keys]\n    \n    def correlate(self, metric1, metric2, start_time, end_time):\n        \"\"\"Find correlation between two metrics\"\"\"\n        data1 = dict(self.rangeQuery(metric1, start_time, end_time))\n        data2 = dict(self.rangeQuery(metric2, start_time, end_time))\n        \n        # Find common timestamps\n        common_times = set(data1.keys()) & set(data2.keys())\n        \n        if len(common_times) < 2:\n            return None\n        \n        # Calculate correlation\n        pairs = [(data1[t], data2[t]) for t in sorted(common_times)]\n        return pairs\n```\n\n**Example Usage:**\n```python\ndb = TimeSeriesDB()\n\n# Write data points\ndb.write(1000, 10)\ndb.write(1010, 15)\ndb.write(1020, 12)\ndb.write(1030, 18)\ndb.write(1040, 20)\n\n# Read specific point\nprint(db.read(1020))  # 12\n\n# Range query\npoints = db.rangeQuery(1010, 1030)\nprint(points)  # [(1010, 15), (1020, 12), (1030, 18)]\n\n# Aggregations\nprint(db.aggregate(1010, 1030, 'avg'))  # 15.0\nprint(db.aggregate(1010, 1030, 'max'))  # 18\nprint(db.aggregate(1010, 1030, 'sum'))  # 45\n\n# Latest points\nprint(db.latest(3))  # Last 3 points\n\n# Downsample\ndownsampled = db.downsample(interval=20)\nprint(downsampled)\n```\n\n**Compression for Large Data:**\n```python\nclass CompressedTSDB:\n    def __init__(self, compression_interval=3600):\n        self.recent = SortedDict()  # Hot data\n        self.compressed = SortedDict()  # Cold data (aggregated)\n        self.compression_interval = compression_interval\n    \n    def write(self, timestamp, value):\n        self.recent[timestamp] = value\n        \n        # Compress old data\n        if len(self.recent) > 10000:\n            self._compress_old_data()\n    \n    def _compress_old_data(self):\n        cutoff = max(self.recent.keys()) - self.compression_interval\n        \n        # Move old data to compressed storage\n        old_keys = [k for k in self.recent if k < cutoff]\n        \n        for key in old_keys:\n            bucket = key // self.compression_interval\n            if bucket not in self.compressed:\n                self.compressed[bucket] = []\n            self.compressed[bucket].append(self.recent[key])\n            del self.recent[key]\n        \n        # Aggregate compressed buckets\n        for bucket in self.compressed:\n            if isinstance(self.compressed[bucket], list):\n                self.compressed[bucket] = {\n                    'avg': sum(self.compressed[bucket]) / len(self.compressed[bucket]),\n                    'max': max(self.compressed[bucket]),\n                    'min': min(self.compressed[bucket]),\n                    'count': len(self.compressed[bucket])\n                }\n```\n\n**Performance Comparison:**\n```\nData Structure      Write     Read      Range     Aggregate\nList                O(1)      O(n)      O(n)      O(n)\nDict                O(1)      O(1)      O(n)      O(n)\nSortedDict          O(log n)  O(log n)  O(log n+k) O(log n+k)\nPandas DataFrame    O(1)      O(log n)  O(k)      O(k)\nRedis TimeSeries    O(1)      O(1)      O(k)      O(k)\n```\n\n**Real-World Features:**\n```python\nclass ProductionTSDB:\n    def __init__(self):\n        self.data = SortedDict()\n        self.metadata = {}  # Store metric metadata\n    \n    def write_batch(self, points):\n        \"\"\"Batch write for efficiency\"\"\"\n        for timestamp, value in points:\n            self.data[timestamp] = value\n    \n    def interpolate(self, timestamp):\n        \"\"\"Get interpolated value if exact time missing\"\"\"\n        if timestamp in self.data:\n            return self.data[timestamp]\n        \n        # Find surrounding points\n        idx = self.data.bisect_left(timestamp)\n        \n        if idx == 0 or idx >= len(self.data):\n            return None  # Out of range\n        \n        # Linear interpolation\n        t1 = self.data.keys()[idx - 1]\n        t2 = self.data.keys()[idx]\n        v1 = self.data[t1]\n        v2 = self.data[t2]\n        \n        # Interpolate\n        ratio = (timestamp - t1) / (t2 - t1)\n        return v1 + ratio * (v2 - v1)\n    \n    def detect_anomalies(self, start, end, threshold=2.0):\n        \"\"\"Simple anomaly detection using z-score\"\"\"\n        points = self.rangeQuery(start, end)\n        values = [v for t, v in points]\n        \n        if len(values) < 3:\n            return []\n        \n        mean = sum(values) / len(values)\n        std = (sum((x - mean) ** 2 for x in values) / len(values)) ** 0.5\n        \n        anomalies = []\n        for t, v in points:\n            z_score = abs((v - mean) / std) if std > 0 else 0\n            if z_score > threshold:\n                anomalies.append((t, v, z_score))\n        \n        return anomalies\n```\n\n**Complexity:**\n- write: O(log n) - balanced tree insert\n- read: O(log n) - binary search\n- rangeQuery: O(log n + k) - binary search + k results\n- aggregate: O(log n + k) - range query + computation\n- Space: O(n) - all data points\n\n**Key Insight:** SortedDict provides optimal balance of write performance and range query efficiency, essential for time-series workloads.",
  "timeComplexity": "O(log n) write, O(log n + k) range",
  "spaceComplexity": "O(n)",
  "videoUrl": "https://www.youtube.com/watch?v=2RofiY2v1iU",
  "videoTitle": "Time Series Database Design (Hussein Nasser)"
}
